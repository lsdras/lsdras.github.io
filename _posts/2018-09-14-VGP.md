---
title: "Vanishing Gradient Problem"
date: 2018-09-14
categories: Vanishing Gradient Problem, ML
---

https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b

위 사이트에서 설명하는 Vanishing gradient problem 내용을 제가 이해한대로 요약해보고자 합니다.

Vanishing gradient problem을 이해하기 위해서는 경사하강법(gradient descent algorithm)과 역전파(back propagation)에 대한 기본적인 이해가 필요합니다.


# 경사하강법(gradient descent)

[경사하강법 알고리즘과 그림 이미지]

우리가 원하는 것은 딥러닝과정에서 정확도의 척도로 쓰이는 cost(loss, error 뭐 다양한 이름으로 불리는 것)를 "최소로 만들어 주는 지점"을 찾는 것 입니다. 

즉, 만약 우리가 딥러닝에서 정의역이 weight, 공역이 cost 인 함수를 생각하면, 이 과정을 통해 우리가 넣어준 input, 즉 러닝에 필요한 데이터셋을 상수취급하여, 그 input들의 계산결과인 cost를 최소로 만들 수 있는 weight를 찾는 과정이 될 것입니다. 


이때, 그 최솟값이 되는 지점에서의 기울기는 0이겠죠. 그렇다면, 임의의 입력값을 제공했을 때, 항상 그 최솟값이 되는 지점을 어떻게 찾을 수 있을까요?

이 경우 가장 기본이 되는 방법이 경사하강법(gradient descent algorithm) 입니다.

함수를 미분하면 그 결과로 나오는 것이 원래 함수의 해당 지점에서의 기울기(gradient)를 함숫값으로 가지는 또 다를 함수입니다.
기울기를 안다는 것은 결국 어느 방향으로 움직여야 현재의 cost값보다 작은 값이 나올지를 알 수 있다는 의미를 가지지요.

예를들어, 현재 임의의 'weight'에서 'cost1'이라는 값이 결과로 나왔으면, 
'weight = weight-(기울기)*a' (a는 임의의 상수로, 기울기 방향으로 얼마나 움직일 것인지를 정해주는 '보폭'과 같은 역할입니다.)
로 weight값을 갱신해줘서 cost2라는 새로운 값이 나오도록 하고, cost2<cost1이 될 것을 기대하는 방법입니다.
이 과정은 기울기 값이 0이 되면 더이상 낮은 값이 없다 판단하여 멈추게 되는 것이지요.

물론 딥러닝 과정에서는 복잡한 함수를 처리하는 과정에서 그 함수의 기울기가 0이 될 때까지 계속 계산을 시키는 것은 computationally expensive 하기때문에 일정 횟수만을 반복하게 하기도 하며, 경사하강법 시행을 통해 찾은 지점의 기울기가 0이지만 그 지점의 값이 최소값은 아닌 local minima, saddle point에 갖혀버리는 문제점 등이 있어서 확률적 경사하강법(Stochastic gradient descent), AbaGrad, Adam, momentum 등의 다른 최적화 알고리즘들 역시 존재합니다.


# 역전파(backpropagation)

오차역전파라고도 불리는 backpropagation은 딥러닝의 핵심적인 개념중 하나라고 생각합니다.
이 과정을 아주아주 생략해서 설명하면 "계산 그래프를 역방향으로 미분해서 각 노드가 결과에 얼마만큼 영향을 끼치는지 파악하는 방법" 이라고 할 수 있습니다.

바로 위에서 경사하강법을 통해 최적해를 찾는 방법에 대해 설명했는데, "그럼 계산그래프로 이뤄진 딥러닝 과정에서 뭘 기울기로 써먹을건데?"에 대한 답변이 이 backpropagation이라고 볼 수 있습니다.

딥러닝 과정의 계산 그래프는 복잡하게 얽혀있기때문에, weight 값이 살짝만 바뀌더라도 결과로 나오는 loss값이 어떻게 변할지는 쉽게 예상할 수 없습니다. 이때, 

더 자세한 내용은 실제 도식화된 그래프를 보면서 여러 예시를 확인하면 더 잘 이해할 수 있으므로, http://cs231n.github.io/optimization-2/, http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf, 밑바닥에서부터 시작하는 딥러닝(한빛미디어) 등의 더 좋은 자료를 참고하시길 바랍니다. 실제 계산그래프를 어떻게 구현하는지에 대해서도 보시면 좋을 것 같습니다.
