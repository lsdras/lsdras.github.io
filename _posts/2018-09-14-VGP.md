---
title: "Vanishing Gradient Problem"
date: 2018-09-14
categories: Vanishing Gradient Problem, ML
---

https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b

위 사이트에서 설명하는 Vanishing gradient problem 내용을 제가 이해한대로 요약해보고자 합니다.

Vanishing gradient problem을 이해하기 위해서는 경사하강법(gradient descent algorithm)과 역전파(back propagation)에 대한 기본적인 이해가 필요합니다.


# 경사하강법(gradient descent)

[경사하강법 알고리즘과 그림 이미지]

우리가 원하는 것은 딥러닝과정에서 정확도의 척도로 쓰이는 cost(loss, error 뭐 다양한 이름으로 불리는 것)를 "최소로 만들어 주는 지점"을 찾는 것 입니다. 

즉, 만약 우리가 딥러닝에서 정의역이 weight, 공역이 cost 인 함수를 생각하면, 이 과정을 통해 우리가 넣어준 input, 즉 러닝에 필요한 데이터셋을 상수취급하여, 그 input들의 계산결과인 cost를 최소로 만들 수 있는 weight를 찾는 과정이 될 것입니다. 


이때, 그 최솟값이 되는 지점에서의 기울기는 0이겠죠. 그렇다면, 임의의 입력값을 제공했을 때, 항상 그 최솟값이 되는 지점을 어떻게 찾을 수 있을까요?

이 경우 가장 기본이 되는 방법이 경사하강법(gradient descent algorithm) 입니다.

함수를 미분하면 그 결과로 나오는 것이 원래 함수의 해당 지점에서의 기울기(gradient)를 함숫값으로 가지는 또 다를 함수입니다.
기울기를 안다는 것은 결국 어느 방향으로 움직여야 현재의 cost값보다 작은 값이 나올지를 알 수 있다는 의미를 가지지요.

예를들어, 현재 임의의 'weight'에서 'cost1'이라는 값이 결과로 나왔으면, 
'weight = weight-(기울기)*a' (a는 임의의 상수로, 기울기 방향으로 얼마나 움직일 것인지를 정해주는 '보폭'과 같은 역할입니다.)
로 weight값을 갱신해줘서 cost2라는 새로운 값이 나오도록 하고, cost2<cost1이 될 것을 기대하는 방법입니다.
이 과정은 기울기 값이 0이 되면 더이상 낮은 값이 없다 판단하여 멈추게 되는 것이지요.

물론 딥러닝 과정에서는 복잡한 함수를 처리하는 과정에서 그 함수의 기울기가 0이 될 때까지 계속 계산을 시키는 것은 computationally expensive 하기때문에 일정 횟수만을 반복하게 하기도 하며, 경사하강법 시행을 통해 찾은 지점의 기울기가 0이지만 그 지점의 값이 최소값은 아닌 local minima, saddle point에 갖혀버리는 문제점 등이 있어서 확률적 경사하강법(Stochastic gradient descent), AbaGrad, Adam, momentum 등의 다른 최적화 알고리즘들 역시 존재합니다.


# 역전파(backpropagation)

오차역전파라고도 불리는 backpropagation은 딥러닝의 핵심적인 개념중 하나라고 생각합니다.
이 과정을 아주아주 생략해서 설명하면 "계산 그래프를 역방향으로 미분해서 각 노드가 결과에 얼마만큼 영향을 끼치는지 파악하는 방법" 이라고 할 수 있습니다.

바로 위에서 경사하강법을 통해 최적해를 찾는 방법에 대해 설명했는데, "그럼 계산그래프로 이뤄진 딥러닝 과정에서 뭘 기울기로 써먹을건데?"에 대한 답변이 이 backpropagation이라고 볼 수 있습니다.

딥러닝 과정의 계산 그래프는 복잡하게 얽혀있기때문에, weight 값이 살짝만 바뀌더라도 결과로 나오는 loss값이 어떻게 변할지는 쉽게 예상할 수 없습니다. 이때, backpropagation은 output에서 input 방향으로 각 연산과정을 연속적으로 미분하여서 "weight들을 하나 빼고 다 고정시킨 다음, 한 weight만 바꿨을 때, output의 변화량"을 바로 확인 할 수 있게 해줍니다.

실제로 딥러닝이라는 것이 정방향으로 cost를 구하고, backpropagation으로 나온 미분값들을 활용해 weight를 수정하고, 다시 정방향 연산을통해 cost를 구하는 과정을 통해 최종적인 cost를 최소화 하는것을 목적으로 하는 것이지요.

더 자세한 내용은 실제 도식화된 그래프를 보면서 여러 예시를 확인하면 더 잘 이해할 수 있으므로, http://cs231n.github.io/optimization-2/, http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf, 밑바닥에서부터 시작하는 딥러닝(한빛미디어) 등의 더 좋은 자료를 참고하시길 바랍니다. 실제 계산그래프를 어떻게 구현하는지에 대해서도 보시면 좋을 것 같습니다.


자 그럼 본론으로 들어가봅시다.

# Vanishing gradient problem 

한국어로 번역하면 '흐려지는, 옅어지는 기울기 문제'정도로 볼 수 있겠네요.

세부적인 내용으로 들어가기 전에 미리 이 문제가 무엇인지 요약하자면, "계산 그래프에서 일부분만을 봤을 때는 미분값에 문제가 없는데, 레이어가 깊어지다보니 이 각각의 값을 연쇄적으로 곱해줘야 했고, 결과적으로 기울기가 너무 작게 나와서 느린 learning속도, underflow 등이 발생하는 문제." 정도로 표현할 수 있습니다.

그렇다면 이 문제가 왜 발생했으며, 어떻게 이 문제를 해결했는지 설명해보겠습니다.

퍼셉트론, 뉴럴넷 모델의 기본적인 구조중에는 활성화 함수라는 요소가 있습니다. 입력받은 값을 연산하고, 출력하기 직전에 변환해주는 함수이죠.

뉴럴넷 모델에서는 비선형함수를 활성화 함수로 이용하는데(XOR 문제를 해결하고, deeper network를 만들기 위해서라고만 요약하겠습니다), 이러한 활성화 함수중 sigmoid 함수를 사용할 때 나타났습니다.

sigmoid 함수는 sigmoid(x)= 1/(1+exp(-x)) 로 정의되며, 모든 실수x를 (0,1)의 범위에 일대일 대응시켜주는 squashing function이기도 합니다.

이때, sidmoid 함수의 도함수값의 범위는 (0,0.25]이고, x=0일 때 최댓값을 가지게 됩니다.

즉, 딥러닝의 backpropagation 과정에서 은닉층의 깊이가 깊어질수록 input쪽으로 갈수록 gradient가 필연적으로 작아질 수 밖에 없는 상황이 발생하고 맙니다. 예를들어, 5개의 hidden layer을 가정해보면, 다른 연산과정에서 아무리 큰 gradient가 발생한다 하더라도, 다섯개의 활성화 함수 때문에 아무리 커도 원래 값의 1/1024배 이하로 작아져버리게 됩니다. 이렇게 gradient가 작아져버리면 위에서 언급했듯이 최소값이 아닌 local minima에 갇혀버리는 문제가 발생하거나, 이런 input 방향에 가까운 layer weight의 부정확성이 결국 뒤쪽 레이어들마저 부정확성을 연쇄적으로 초래하므로 뉴럴넷 구조 자체를 망치게됩니다. 느린 학습속도도 마찬가지고요.

뿐만 아니라, 구조가 상당히 깊다면 이진 계산기의 본질적 한계점인 underflow 가 발생할 가능성도 있고요.

이 문제는 sigmoid 뿐만 아니라, sigmoid를 변형시킨 tanh 함수는 도함수값의 범위가 (0,1]인지라 sigmoid보다 낫긴 하지만, 역시 vanishing gradient에서 자유롭진 못합니다.

결국 이 vanishing gradient의 궁극적 원인은 "활성화함수의 작은 도함수값" 때문에 발생했다고 요약할 수 있을 겁니다.

그렇다면 도함수값을 일정하게 만들면 이 문제를 해결할 수 있지 않을까? 라는 발상으로 최근 많이 사용되는 활성화 함수가 ReLU(x)=max(0,x)입니다.![ReLU Paper](https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf)



그렇다면 왜 굳이 이런 문제점을 안고 있는 sigmoid 함수를 쓴 것인가?
