---
title: "Vanishing Gradient Problem"
date: 2018-09-14
categories: Vanishing Gradient Problem, ML
---

https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b

위 사이트에서 설명하는 Vanishing gradient problem 내용을 제가 이해한대로 요약해보고자 합니다.

Vanishing gradient problem을 이해하기 위해서는 경사하강법(gradient descent algorithm)과 역전파(back propagation)에 대한 기본적인 이해가 필요합니다.


# 경사하강법(gradient descent algorithm)

[경사하강법 알고리즘과 그림 이미지]

우리가 원하는 것은 딥러닝과정에서 정확도의 척도로 쓰이는 cost(loss, error 뭐 다양한 이름으로 불리는 것)를 "최소로 만들어 주는 지점"을 찾는 것 입니다. 

만약 우리가 딥러닝에서 정의역이 weight, 공역이 cost 인 함수를 생각하면, 이 과정을 통해 우리가 넣어준 input, 즉 러닝에 필요한 데이터셋을 상수취급하여, 그 input들의 cost를 최소로 만들 수 있는 weight를 찾는 과정이 될 것입니다. 


이때, 그 최솟값이 되는 지점에서의 기울기는 0이겠죠. 그렇다면, 임의의 입력값을 제공했을 때, 항상 그 최솟값이 되는 지점으로 
