---
title: "Vanishing Gradient Problem"
date: 2018-09-14
categories: Vanishing Gradient Problem, ML
---

https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b

위 사이트에서 설명하는 Vanishing gradient problem 내용을 제가 이해한대로 요약해보고자 합니다.

Vanishing gradient problem을 이해하기 위해서는 경사하강법(gradient descent algorithm)과 역전파(back propagation)에 대한 기본적인 이해가 필요합니다.


# 경사하강법(gradient descent)

[경사하강법 알고리즘과 그림 이미지]

우리가 원하는 것은 딥러닝과정에서 정확도의 척도로 쓰이는 cost(loss, error 뭐 다양한 이름으로 불리는 것)를 "최소로 만들어 주는 지점"을 찾는 것 입니다. 

즉, 만약 우리가 딥러닝에서 정의역이 weight, 공역이 cost 인 함수를 생각하면, 이 과정을 통해 우리가 넣어준 input, 즉 러닝에 필요한 데이터셋을 상수취급하여, 그 input들의 계산결과인 cost를 최소로 만들 수 있는 weight를 찾는 과정이 될 것입니다. 


이때, 그 최솟값이 되는 지점에서의 기울기는 0이겠죠. 그렇다면, 임의의 입력값을 제공했을 때, 항상 그 최솟값이 되는 지점을 어떻게 찾을 수 있을까요?

이 경우 가장 기본이 되는 방법이 경사하강법(gradient descent algorithm) 입니다.

함수를 미분하면 그 결과로 나오는 것이 원래 함수의 해당 지점에서의 기울기(gradient)를 함숫값으로 가지는 또 다를 함수입니다.
기울기를 안다는 것은 결국 어느 방향으로 움직여야 현재의 cost값보다 작은 값이 나올지를 알 수 있다는 의미를 가지지요.

즉, 현재 임의의 'weight'에서 'cost1'이라는 값이 결과로 나왔으면, 
'weight = weight-(기울기)*a' (a는 임의의 상수로, 기울기 방향으로 얼마나 움직일 것인지를 정해주는 '보폭'과 같은 역할입니다.)
로 weight값을 갱신해줘서 cost2라는 새로운 값이 나오도록 하고, cost2<cost1이 될 것을 기대하는 방법입니다.
이 과정은 기울기 값이 0이 되면 더이상 낮은 값이 없다 판단하여 멈추게 되는 것이지요.

물론 딥러닝 과정에서는 복잡한 함수를 처리하는 과정에서 그 함수의 기울기가 0이 될 때까지 계속 계산을 시키는 것은 computationally expensive 하기때문에 일정 횟수만을 반복하게 하기도 하며, 경사하강법 시행을 통해 찾은 지점의 기울기가 0이지만 그 지점의 값이 최소값은 아닌 local minima, saddle point에 갖혀버리는 문제점 등이 있어서 확률적 경사하강법(Stochastic gradient descent), AbaGrad, Adam, momentum 등의 다른 최적화 알고리즘들 역시 존재합니다.


# 역전파(back propagation)

